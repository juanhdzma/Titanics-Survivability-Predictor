{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/juanhdzma/titanic-s-survivability-predictor-ann-0-7655?scriptVersionId=129561179\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **Titanic's survivability predictor**\n#### By: Juan Carlos Hernandez Mari√±o","metadata":{"id":"QtBlPnkius8P"}},{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"The purpose of the following project is to predict and classify whether a certain group of passengers survived the Titanic accident. To achieve this goal, various techniques from Machine Learning, Deep Learning, and Data Engineering will be utilized. Some of the techniques that will be employed include:\n\n* Machine Learning:\n  * K-Nearest Neighbors (KNN)\n  * Logistic Regression (LR)\n  * Support Vector Machine (SVM)\n* Deep Learning:\n  * Artificial Neural Networks (ANN)\n* Data Engineering:\n  * Data Cleaning\n  * Data Transformation\n\nIn order to achieve the stated goal, we will evaluate and compare the predictions generated by each algorithm (Logistic Regression, SVM, and Neural Networks) to assess their respective performance. The classification task will involve a binary variable, where '1' indicates that the passenger survived and '0' indicates that the passenger did not survive.\n\nTo gain a better understanding of the problem, we will now proceed with a brief overview of the dataset and its values.\n\n\n\n\n\n\n\n","metadata":{"id":"SEo1wvN6vWbL"}},{"cell_type":"markdown","source":"## Dataset and Exploratory Data Analysis (EDA)","metadata":{"id":"U979gvEMvR-T"}},{"cell_type":"code","source":"import pandas as pd\n\ndataset = pd.read_csv('/kaggle/input/titanic/train.csv')\ndataset.head()","metadata":{"id":"cn3uJJMjitQj","outputId":"4a753f93-64e7-40ad-844f-b7a30553d904","execution":{"iopub.status.busy":"2023-05-14T20:06:39.19479Z","iopub.execute_input":"2023-05-14T20:06:39.195151Z","iopub.status.idle":"2023-05-14T20:06:39.27378Z","shell.execute_reply.started":"2023-05-14T20:06:39.195121Z","shell.execute_reply":"2023-05-14T20:06:39.272516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### *Metadata*\nMeaning and possible values of each attribute:\n\n**PassengerId**: Index of passangers (Positive integer)\n\n**Survived**: State of the passanger (1, 0), '1' Meaning that the passanger survived and '0' that not.\n\n**Pclass**: Ticket class (1, 2, 3), each one considered as first, second, or third class.\n\n**Name**: Name of the passanger (String)\n\n**sex**: Biological sex (male, female)\n\n**Age**: Age in years (Positive float)\n\n**SibSp**: # of siblings or spouses aboard the Titanic (Positive integer)\n    \n**Parch**: # of parents or children aboard the Titanic (Positive integer)\n\n**Ticket**: Ticket number (String)\n    \n**fare**: Ticket fare (Positive float)\n\n**cabin**: Cabin number (String)\n    \n**embarked**: Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)","metadata":{}},{"cell_type":"markdown","source":"Based on the information provided, we have determined the data types associated with each column. The 'PassengerId' attribute, which serves as an index value, will be disregarded for further analysis. Similarly, the 'Name' attribute will not be considered due to its intricate nature and the complexity involved in its thorough analysis. Given these exclusions, we can proceed with a comprehensive description of the data, starting the number of instances in the dataset and the numerical attributes. ","metadata":{}},{"cell_type":"code","source":"print(f\"Number of instances in the dataset: {dataset.shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-14T20:06:39.275282Z","iopub.execute_input":"2023-05-14T20:06:39.275566Z","iopub.status.idle":"2023-05-14T20:06:39.281986Z","shell.execute_reply.started":"2023-05-14T20:06:39.275542Z","shell.execute_reply":"2023-05-14T20:06:39.280624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.drop(columns=['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True, errors='ignore')\n\n## Temporarily, we will exclude the attributes 'Survived' and 'Pclass' from our analysis as they are classified as categorical variables.\n\ndataset.drop(columns=['Survived', 'Pclass'], axis=1).describe()","metadata":{"execution":{"iopub.status.busy":"2023-05-14T20:06:39.283571Z","iopub.execute_input":"2023-05-14T20:06:39.283851Z","iopub.status.idle":"2023-05-14T20:06:39.319523Z","shell.execute_reply.started":"2023-05-14T20:06:39.283829Z","shell.execute_reply":"2023-05-14T20:06:39.318508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this information have some important insights:\n\n* **Age**: The majority of passengers on the Titanic were below the age of *38*.\n* **SibSp**: Most passengers did not have their spouses or siblings accompanying them on board.\n* **Parch**: The vast majority of passengers did not have their parents or children with them on board.\n* **Fare**: There is a significant variation in the fare paid by passengers. The mean fare is *32.20*, with the maximum fare reaching *512.32*. The fare range is considerable, spanning from *0* to *512.32*.\n\nHaving analyzed the numerical attributes, we will now proceed with the examination of the categorical variables.","metadata":{}},{"cell_type":"code","source":"## Temporarily, we will exclude the attributes 'Age', 'SibSp', 'Parch', and 'Fare' from our analysis as they are classified as numerical variables.\n\nfor item in dataset.drop(columns=['Age', 'SibSp', 'Parch', 'Fare', 'Cabin'], axis=1):   \n    print(f\"==={item.center(10)}===\")\n    counts_table = dataset[item].value_counts().reset_index()\n    counts_table.columns = ['Value', 'Count']\n    print(counts_table, end='\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2023-05-14T20:06:39.321798Z","iopub.execute_input":"2023-05-14T20:06:39.322859Z","iopub.status.idle":"2023-05-14T20:06:39.34498Z","shell.execute_reply.started":"2023-05-14T20:06:39.322801Z","shell.execute_reply":"2023-05-14T20:06:39.343088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the previous analysis, we excluded the attributes 'Ticket' and 'Cabin' as they contain values that serve as identifiers for specific cabins or tickets, limiting exploratory analysis.\n\nFrom this information have some important insights:\n* **Survived**: The majority of passengers did not survive.\n* **Pclass**: As the class increases, the number of passengers decreases.\n* **Sex**: The majority of passengers are male.\n* **Embarked**: The vast majority of passengers boarded the ship at the port of Southampton (S).\n\nConsidering the information provided, it is advisable to assess the presence of null values in the dataset to incorporate them into the data cleaning process. This step is crucial for handling missing data effectively during data preprocessing.","metadata":{}},{"cell_type":"code","source":"dataset.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-14T20:06:39.346499Z","iopub.execute_input":"2023-05-14T20:06:39.346836Z","iopub.status.idle":"2023-05-14T20:06:39.35669Z","shell.execute_reply.started":"2023-05-14T20:06:39.346808Z","shell.execute_reply":"2023-05-14T20:06:39.355519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the obtained results, it is necessary to take into account the attributes 'Age', 'Cabin', and 'Embarked' due to the presence of null values in certain instances.\n\nAfter conducting some exploration of the dataset, it is crucial to visually examine if any patterns can be identified between each column and the target variable 'Survived'. This analysis will help uncover potential relationships and insights that can contribute to the prediction and classification of survival outcomes.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncategorical_attributes = ['Pclass', 'Sex', 'Embarked']\nfig, axes = plt.subplots(nrows=len(categorical_attributes), ncols=1, figsize=(8, len(categorical_attributes)*5))\n\nfor i, attribute in enumerate(categorical_attributes):\n    ax = axes[i]\n    grouped_data = dataset.groupby([attribute, 'Survived']).size().unstack()\n    percentage_data = grouped_data.div(grouped_data.sum(axis=1), axis=0) * 100\n    percentage_data.plot(kind='bar', stacked=False, ax=ax)\n    ax.set_xlabel(attribute)\n    ax.set_ylabel('Percentage')\n    ax.set_title(f'Percentages of Passengers that Survived by {attribute}')\n    ax.legend(title='Survived')\n    \n    for container in ax.containers:\n        ax.bar_label(container, label_type='center', fmt='%.1f%%', padding=0.4)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-14T20:07:58.544071Z","iopub.execute_input":"2023-05-14T20:07:58.544409Z","iopub.status.idle":"2023-05-14T20:07:59.333987Z","shell.execute_reply.started":"2023-05-14T20:07:58.544383Z","shell.execute_reply":"2023-05-14T20:07:59.333217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_attributes = ['Age', 'SibSp', 'Parch', 'Fare']\nfig, axes = plt.subplots(nrows=len(numeric_attributes), figsize=(8, len(numeric_attributes)*4))\n\nfor i, attribute in enumerate(numeric_attributes):\n    ax = axes[i]\n    sns.violinplot(data=dataset, x='Survived', y=attribute, ax=ax)\n    ax.set_xlabel('Survived')\n    ax.set_ylabel(attribute)\n    ax.set_title(f'{attribute} by Survived')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-14T20:08:02.218784Z","iopub.execute_input":"2023-05-14T20:08:02.219165Z","iopub.status.idle":"2023-05-14T20:08:02.906412Z","shell.execute_reply.started":"2023-05-14T20:08:02.219134Z","shell.execute_reply":"2023-05-14T20:08:02.905609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a clear relationship between the 'Pclass' attribute and the survival rate, it can be observed that a higher percentage of people survived in the first class compared to the third class. This trend is also evident in the 'Fare' attribute, where passengers who paid higher fares have a greater likelihood of survival.\n\nFurthermore, it is evident in the 'Sex' attribute that the majority of females survived compared to males.\n\nOn the other hand, there is a higher survival rate for passengers who embarked at port C.\n\nWhile there are other visible relationships in the analysis, the most relevant ones are the ones mentioned above.","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"For data preprocessing, we will focus on two main steps: data cleaning and transformation.\n* **Data Cleaning**:\n    * ***Handling Missing Values***: We know that there are some missing values in the attributes 'Age', 'Cabin' and 'Embarked'. \n        * For the 'Age' attribute, which has a significant number of missing values, we will employ the K Nearest Neighbors (KNN) algorithm. We will identify the five nearest neighbors for each missing value and assign the missing value as the mean of the 'Age' values of its nearest neighbors.\n        * Regarding the 'Cabin' attribute, we will simplify the approach by converting it into a binary variable. We will assign '1' to instances with a valid 'Cabin' value and '0' to instances with missing cabin values.\n        * As for the 'Embarked' attribute, since there are only two instances with missing values, we will simply remove those instances from the dataset.\n    * ***Dropping unused attributes***: We will drop certain attributes from the dataset as they either serve as index values or are too complex to conduct further analysis. The attributes 'PassengerId', 'Name', and 'Ticket' will be removed from the dataset.\n    \n    \n* **Data Transformation**:\n    * ***Feature Scaling***: To ensure that the numerical attributes have the same scale, we will perform feature scaling using the MinMaxScaler transformation. This technique will be applied to the attributes 'Pclass', 'Age', 'SibSp', 'Parch', and 'Fare'. The MinMaxScaler will normalize the values of these attributes between 0 and 1. This will prevent any particular attribute from dominating the analysis due to a larger scale.\n    * ***Encoding Categorical Variables***: Categorical variables need to be encoded into numerical form to be used in machine learning algorithms.\n        * For the 'Sex' and 'Cabin' attributes, which have binary values, we will assign '1' and '0' for their categories.\n        * For the 'Embarked' attribute, which has three possible values (C, Q, S), we will use one-hot encoding. This means creating three separate binary variables, where each variable represents one possible value. For an instance, if it is associated with the 'C' category, the 'C' variable will be set to '1', and the other variables ('Q' and 'S') will be set to '0'.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import KNNImputer\n\ntrainData = pd.read_csv('/kaggle/input/titanic/train.csv')\ntestData = pd.read_csv('/kaggle/input/titanic/test.csv')\n\ndef getPredictorData(db):\n    ## Deep copy of the dataset\n    dataset = db.copy()\n    ## Drop unused columns\n    dataset.drop(columns=['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True, errors='ignore') \n    \n    ## Categorical Values\n    ## Categorize the 'Cabin' attribute\n    dataset['Cabin'] = dataset['Cabin'].notnull().astype(int)\n    ## Categorize the 'Sex' attibute\n    dataset['Sex'] = dataset['Sex'].map({'male': 0, 'female': 1})\n    ## Delete the null 'Embarked' values, then do the one-hot encoding and annex it to the dataset\n    dataset.dropna(subset=['Embarked'], inplace=True)\n    one_hot_encoded = pd.get_dummies(dataset['Embarked'], prefix='Embarked')\n    dataset = pd.concat([dataset, one_hot_encoded], axis=1)\n    dataset.drop('Embarked', axis=1, inplace=True)\n\n    ## Numerical Values\n    ## 'Age' Posterior imputation\n    imputer = KNNImputer(n_neighbors=5)\n    dataset['Age'] = imputer.fit_transform(dataset[['Age']])\n    dataset['Age'] = dataset['Age'].astype(int)\n    ## MinMax Scaler in 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare' attributes\n    columns_to_normalize = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n    subset = dataset[columns_to_normalize]\n    scaler = MinMaxScaler()\n    normalized_data = scaler.fit_transform(subset)\n    normalized_df = pd.DataFrame(normalized_data, columns=columns_to_normalize)\n    dataset[columns_to_normalize] = normalized_df\n    \n    ## Droping any left value\n    dataset.dropna(how='any', inplace=True)\n    \n    ## Copy the target from 'Survived' attribute\n    target = dataset['Survived'].copy()\n    ## Delete the attribute 'Survived' from the dataset\n    dataset.drop(columns=['Survived'], axis=1, inplace=True, errors='ignore') \n    return dataset, target","metadata":{"id":"qLbOopLgkC8L","outputId":"5d3e4493-e252-4539-fecb-de3d79472b2d","execution":{"iopub.status.busy":"2023-05-14T20:30:56.254926Z","iopub.execute_input":"2023-05-14T20:30:56.255266Z","iopub.status.idle":"2023-05-14T20:30:56.276203Z","shell.execute_reply.started":"2023-05-14T20:30:56.25524Z","shell.execute_reply":"2023-05-14T20:30:56.275523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ML and DL modeling","metadata":{}},{"cell_type":"markdown","source":"We will compare the results of three different machine and deep learning algorithms: Logistic Regression, Support Vector Machine (SVM), and Artificial Neural Networks (ANN). These algorithms will be trained and evaluated on the given dataset to predict the target variable, 'Survived'.\n\nTo compare the performance of the Logistic Regression, Support Vector Machine (SVM), and Artificial Neural Networks (ANN) models, we will randomly divide the training dataset into training and testing sets. Each model will be trained and evaluated 10 times on different random divisions. We will calculate the average performance across the repetitions to determine which model performs better overall.","metadata":{}},{"cell_type":"markdown","source":"#### **Artificial Neural Network (ANN)**","metadata":{}},{"cell_type":"code","source":"from typing import List\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n## Function to generate a model given the number of neurons per layer.\n## Also, with the number of items in the list we can deduce the number of hidden layers\ndef regression_model(neurons: List[int], numCols: int):\n    model = Sequential()\n    ## We iterate over each layer we want to create and create it\n    for index, numNeurons in enumerate(neurons):\n      ## If the layer is the first one, we define the number of input neurons\n      if index == 0:\n        model.add(Dense(numNeurons, activation='relu', input_shape=(numCols,)))\n      else:\n        model.add(Dense(numNeurons, activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model\n\n## Define a function to train a model\ndef ANNTraining(neurons: List[int], numCols: int, predictors, target, epochs):\n    ## We create a NN with 'len(neurons)'' hidden layers and the 'neurons' list provides the number of neurons in each layer\n    ann = regression_model(neurons, numCols)\n    ## Fit the model with de train data, with a epochs of 50, and verbose = 0 to avoid displaying the default prints\n    ann.fit(predictors, target, epochs=epochs, verbose=0)\n    return ann\n","metadata":{"execution":{"iopub.status.busy":"2023-05-14T20:14:31.156736Z","iopub.execute_input":"2023-05-14T20:14:31.157121Z","iopub.status.idle":"2023-05-14T20:14:31.165679Z","shell.execute_reply.started":"2023-05-14T20:14:31.15709Z","shell.execute_reply":"2023-05-14T20:14:31.164156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Logistic Regression (LR)**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\ndef LRTraining(predictors, target):\n    ## Create Logistic Regression model with liblinear solver\n    lr = LogisticRegression(solver='liblinear')\n    lr.fit(predictors, target)\n    return lr","metadata":{"execution":{"iopub.status.busy":"2023-05-14T20:14:11.927143Z","iopub.execute_input":"2023-05-14T20:14:11.927548Z","iopub.status.idle":"2023-05-14T20:14:11.93281Z","shell.execute_reply.started":"2023-05-14T20:14:11.927518Z","shell.execute_reply":"2023-05-14T20:14:11.931814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Support Vector Machine (SVM)**","metadata":{}},{"cell_type":"code","source":"from sklearn import svm\n\ndef SVMTraining(predictors, target):\n    ## Create SVM model with 'rbf' kernel\n    clf = svm.SVC(kernel='rbf')\n    clf.fit(predictors, target) \n    return clf","metadata":{"execution":{"iopub.status.busy":"2023-05-14T20:16:57.970291Z","iopub.execute_input":"2023-05-14T20:16:57.971766Z","iopub.status.idle":"2023-05-14T20:16:57.978821Z","shell.execute_reply.started":"2023-05-14T20:16:57.971715Z","shell.execute_reply":"2023-05-14T20:16:57.976997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Comparison**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import jaccard_score\n\ndef executeModel(model, predictors, target, iterations):\n    accuracy, f1, jaccard = 0, 0 ,0\n    for index in range(iterations):\n        ## Divide the dataset\n        X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3)\n        \n        ## Artificial Neural Networks\n        if model == 'ANN':\n            annModel = ANNTraining([10, 10], len(predictors.columns), X_train, y_train, 100)\n            ## Predict the values for X_test \n            predictions = annModel.predict(X_test, verbose=0)\n            predictions = [1 if value > 0.5 else 0 for value in predictions]\n            ## Aggregate metrics\n            accuracy += accuracy_score(y_test, predictions)\n            f1 += f1_score(y_test, predictions)\n            jaccard += jaccard_score(y_test, predictions)\n            \n        ## Logistic Regression\n        elif model == 'LR':\n            lrModel = LRTraining(X_train, y_train)\n            ## Predict the values for X_test \n            predictions = lrModel.predict(X_test)\n            ## Aggregate metrics\n            accuracy += accuracy_score(y_test, predictions)\n            f1 += f1_score(y_test, predictions)\n            jaccard += jaccard_score(y_test, predictions)\n                \n        ## Support Vector Machine\n        elif model == 'SVM':\n            svmModel = SVMTraining(X_train, y_train)\n            ## Predict the values for X_test \n            predictions = svmModel.predict(X_test)\n            ## Aggregate metrics\n            accuracy += accuracy_score(y_test, predictions)\n            f1 += f1_score(y_test, predictions)\n            jaccard += jaccard_score(y_test, predictions)\n            \n    return [model, (accuracy/iterations), (f1/iterations), (jaccard/iterations)]","metadata":{"execution":{"iopub.status.busy":"2023-05-14T20:23:44.357038Z","iopub.execute_input":"2023-05-14T20:23:44.35739Z","iopub.status.idle":"2023-05-14T20:23:44.368078Z","shell.execute_reply.started":"2023-05-14T20:23:44.357361Z","shell.execute_reply":"2023-05-14T20:23:44.366462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = ['ANN', 'LR', 'SVM']\ndata = []\n\nfor index, value in enumerate(models):\n    predictors, target = getPredictorData(trainData)\n    result = executeModel(value, predictors, target, 10)\n    data.append(result)\n    \ncomparison = pd.DataFrame(data, columns=['Model', 'Accuracy', 'F1', 'Jaccard'])\nprint(comparison)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T20:24:44.499284Z","iopub.execute_input":"2023-05-14T20:24:44.499681Z","iopub.status.idle":"2023-05-14T20:25:29.343677Z","shell.execute_reply.started":"2023-05-14T20:24:44.499652Z","shell.execute_reply":"2023-05-14T20:25:29.342743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"markdown","source":"First we need to do a data engineering process in the test dataset.","metadata":{}},{"cell_type":"code","source":"## Data treatment to the Test dataset\n\ndataset = testData.copy()\ndataset.drop(columns=['Name', 'Ticket'], axis=1, inplace=True, errors='ignore') \ndataset['Cabin'] = dataset['Cabin'].notnull().astype(int)\ndataset['Sex'] = dataset['Sex'].map({'male': 0, 'female': 1})\none_hot_encoded = pd.get_dummies(dataset['Embarked'], prefix='Embarked')\ndataset = pd.concat([dataset, one_hot_encoded], axis=1)\ndataset.drop('Embarked', axis=1, inplace=True)\nimputer = KNNImputer(n_neighbors=5)\ndataset['Age'] = imputer.fit_transform(dataset[['Age']])\ndataset['Age'] = dataset['Age'].astype(int)\nimputer = KNNImputer(n_neighbors=5)\ndataset['Fare'] = imputer.fit_transform(dataset[['Fare']])\ndataset['Fare'] = dataset['Fare'].astype(int)\ncolumns_to_normalize = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nsubset = dataset[columns_to_normalize]\nscaler = MinMaxScaler()\nnormalized_data = scaler.fit_transform(subset)\nnormalized_df = pd.DataFrame(normalized_data, columns=columns_to_normalize)\ndataset[columns_to_normalize] = normalized_df\nids = dataset['PassengerId'].copy()\ndataset.drop('PassengerId', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T20:39:24.966739Z","iopub.execute_input":"2023-05-14T20:39:24.967145Z","iopub.status.idle":"2023-05-14T20:39:25.00738Z","shell.execute_reply.started":"2023-05-14T20:39:24.9671Z","shell.execute_reply":"2023-05-14T20:39:25.005382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After this, select a model to predict.","metadata":{}},{"cell_type":"code","source":"## Training dataset\npredictors, target = getPredictorData(trainData)\n\n## ANN Model\nannModel = ANNTraining([10, 10], len(predictors.columns), predictors, target, 100)\n## Predict the values for Test \npredictions = annModel.predict(dataset, verbose=0)\npredictions = [1 if value > 0.5 else 0 for value in predictions]\n\n# ## LR Model\n# lrModel = LRTraining(predictors, target)\n# ## Predict the values for Test \n# predictions = lrModel.predict(dataset)\n\n# ## SVM Model\n# svmModel = SVMTraining(predictors, target)\n# ## Predict the values for Test \n# predictions = svmModel.predict(dataset)\n\nresult = list(zip(ids, predictions))\nresult = pd.DataFrame(result, columns=['PassengerId', 'Survived'])\nresult.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T20:56:20.920549Z","iopub.execute_input":"2023-05-14T20:56:20.920898Z","iopub.status.idle":"2023-05-14T20:56:26.034708Z","shell.execute_reply.started":"2023-05-14T20:56:20.92087Z","shell.execute_reply":"2023-05-14T20:56:26.033394Z"},"trusted":true},"execution_count":null,"outputs":[]}]}